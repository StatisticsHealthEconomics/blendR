---
title: "Incorporating different types of external survival data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{external-data}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, warning=FALSE, message=FALSE}
library(blendR)
```

```{r, warning=FALSE, message=FALSE}
library(survHE)
library(INLA)
```

```{r limit-cores, echo=FALSE}
# R CMD check only allows a maximum of two cores
options(mc.cores=2)
inla.setOption(num.threads = 2)
```

## Introduction

To recap, the blended survival curve approach required two survival curves as inputs which are principally combined into a single curve.
Most generally, the method does not care where these curve have come from as long as they are legitimate survival curves.
In fact, we suppose that one of the curves represent observed data and the other some _external_ information which could come from a related trial or expert elicitation.
This then begs the questions, what is the format of the elicited data and how is it transformed to a survival curve as input to the blended survival model?

## Observed survival model

Following from the example in the introductory vignette let us again use the blendR provided data set and fit using an INLA piece-wise exponential model.

First load the package data and take a look.

```{r}
data("TA174_FCR", package = "blendR")
head(dat_FCR)
```

Next, fit the observed data with a piece-wise exponential model using INLA.

```{r}
## observed estimate
obs_Surv <- fit_inla_pw(data = dat_FCR,
                        cutpoints = seq(0, 180, by = 5))
```


## External data as landmark times and survival probabilities

The first approach is to simply define survival probabilities at discrete times.

### Synthetic data set

Using this information then we create some synthetic data consistent with these user-defined constraints using the `ext_surv_sim()` function.
The degree of uncertainty about these probabilities is represented by the sample size of the synthetic data set;
smaller sample sizes indicate more uncertainty and larger sample sizes are greater certainty.

```{r}
data_sim <- ext_surv_sim(t_info = 144,
                         S_info = 0.05,
                         T_max = 180,
                         n = 100)
```

Then fit a gompertz model to this, as follows.

```{r}
ext_Surv <- fit.models(formula = Surv(time, event) ~ 1,
                       data = data_sim,
                       distr = "gompertz",
                       method = "hmc",
                       priors = list(gom = list(a_alpha = 0.1,
                                                b_alpha = 0.1)))
```

Lastly, we can run the blending step.

```{r}
blend_interv <- list(min = 48, max = 150)
beta_params <- list(alpha = 3, beta = 3)

ble_Surv <- blendsurv(obs_Surv, ext_Surv, blend_interv, beta_params)
```

We can visualise all of the curves.

```{r}
plot(ble_Surv)
```


## Piecewise uniform CDF

In the example above we sample from separate uniform distributions in between the times specified by the user.
These data are then used to fit a parametric survival curve which is use in the blending step.

As an alternative - rather than fitting to sampled data - we could simply use the underlying survival curve directly i.e. the 'curve' consisting of uniform segments.

We provide the function `pw_unif_surv()` to create a single piecewise uniform survival curve.

```{r}
p <- c(0.05, 0)
times <- c(144, 180)
surv <- pw_unif_surv(p, times, epsilon = 1)

plot(surv, type = "l")
```

We can now simply blend the two curves together in the usual way.
Because this is a single curve then there is no uncertainty associated with it.

```{r}
blend_interv <- list(min = 48, max = 150)
beta_params <- list(alpha = 3, beta = 3)

ble_Surv <- blendsurv(obs_Surv, surv$S, blend_interv, beta_params)

plot(ble_Surv)
```

Further, one way in which we can include uncertainty on the external curve is by sampling from a Dirichlet distribution on the user-provided survival probabilities.

$$
{\bf p} \sim Dirichlet({\bf \alpha})
$$


If we use the probabilities as the parameter values of the Dirichlet distribution then the degree of certainty can be thought of as a scaling factor on these.
That is for a larger scaling, i.e. taking a product with a large value, is equivalent to more certainty and vice-versa using a smaller scaling is equivalent to more uncertainty.
Intuitively in a Bayesian context, we can view a hyperprior vector $\alpha$ as pseudocounts, i.e. as representing the number of observations in each category that we have already seen.

First let us see for a moderate degree of uncertainty. We will sample `n=100` curves.

```{r}
blend_interv <- list(min = 30, max = 50)
beta_params <- list(alpha = 3, beta = 3)

surv <- sample_pw_unif_surv(p = c(0.8, 0.3, 0.2, 0),
                            times = c(10, 20, 30, 48),
                            n = 100, epsilon = 0.5)

matplot(surv, type = "l")

ble_Surv <- blendsurv(obs_Surv, surv, blend_interv, beta_params)

plot(ble_Surv)
```

Explicitly setting the scaling factor `sn=100` we can repeat analysis this for a relatively small amount of uncertainty.

```{r}
surv <- sample_pw_unif_surv(p = c(0.8, 0.3, 0.2, 0),
                            times = c(10, 20, 30, 48),
                            n = 100, epsilon = 0.5, sn = 100)

matplot(surv, type = "l")

ble_Surv <- blendsurv(obs_Surv, surv, blend_interv, beta_params)

plot(ble_Surv)
```

For a large amount of uncertainty with `sn=3`.

```{r}
surv <- sample_pw_unif_surv(p = c(0.8, 0.3, 0.2, 0),
                            times = c(10, 20, 30, 48),
                            n = 100, epsilon = 0.5, sn = 3)

matplot(surv, type = "l")

ble_Surv <- blendsurv(obs_Surv, surv, blend_interv, beta_params)

plot(ble_Surv)
```


## External data at landmark times as risk sets and number of events

This is related to the approach above but can be thought of as more general.
This is a format taken from Jackson and his `survextrap` package (https://github.com/chjackson/survextrap/).
The idea is that at discrete pairs of time points risk set sizes and then subsequent number of events (observed via the change in the risk set) are defined. These can be overlapping with other risk sets and event counts.
This format allows different sizes of risk sets to be specified at different points of time which can represent the uncertainty with the values in the data.

blendR currently has two experimental functions for handling this sort of input data: `transform_extdata()` and `riskset_surv_sim()`.

```{r}
# transform_extdata()
# riskset_surv_sim()
```

```{r}
# install.packages("survextrap", repos=c('https://chjackson.r-universe.dev', 'https://cloud.r-project.org'))
library(survextrap)
library(ggplot2)
library(dplyr)
```

```{r}
survminer::ggsurvplot(survfit(Surv(time, event) ~ 1, data=data_sim), data=data_sim)

nd_mod1 <- survextrap(Surv(time, event) ~ 1, data=data_sim, chains=1)
plot(nd_mod1, tmax=200)
```

```{r}
extdata <- data.frame(start = 80, stop = 100,
                      n = 100, r = 10)
nd_mod2 <- survextrap(Surv(time, event) ~ 1, data=data_sim, chains=1, external = extdata)
plot(nd_mod2, tmax=200)
```

```{r eval=FALSE}
surv_med1 <- survextrap::survival(nd_mod1)     # median, upper, lower
surv_med2 <- survextrap::survival(nd_mod2)

ble_Surv11 <- blendsurv(obs_Surv, surv_med1$median, blend_interv, beta_params)
ble_Surv12 <- blendsurv(obs_Surv, surv_med2$median, blend_interv, beta_params)

plot(ble_Surv11)
plot(ble_Surv12)
```

```{r eval=FALSE}
surv_mcmc1 <- survextrap::survival(nd_mod1, sample = TRUE)  # MCMC sample
surv_mcmc2 <- survextrap::survival(nd_mod2, sample = TRUE)

ble_Surv21 <- blendsurv(obs_Surv, surv_mcmc1, blend_interv, beta_params)
ble_Surv22 <- blendsurv(obs_Surv, surv_mcmc2, blend_interv, beta_params)

plot(ble_Surv21)
plot(ble_Surv22)
```

