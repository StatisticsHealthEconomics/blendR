---
title: "Basic examples"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{basic}
  %\VignetteEncoding{UTF-8}
  %\VignetteEngine{knitr::rmarkdown}
editor_options: 
  chunk_output_type: console
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup, warning=FALSE, message=FALSE}
library(blendR)
```

```{r, warning=FALSE, message=FALSE}
library(survHE)
library(INLA)
```

```{r limit-cores, echo=FALSE}
# R CMD check only allows a maximum of two cores
options(mc.cores=2)
inla.setOption(num.threads = 2)
```


## INLA piece-wise exponential and external knowledge

First load the package data and take a look.

```{r}
data("TA174_FCR", package = "blendR")
head(dat_FCR)
```

Next, fit the observed data with a piece-wise exponential model using INLA.

```{r}
## observed estimate
obs_Surv <- fit_inla_pw(data = dat_FCR,
                        cutpoints = seq(0, 180, by = 5))
```

For the external model first we create some synthetic data consistent with user-defined constraints as follows.

```{r}
## external estimate
data_sim <- ext_surv_sim(t_info = 144,
                         S_info = 0.05,
                         T_max = 180)
```

Then fit a gompertz model to this.

```{r}
ext_Surv <- fit.models(formula = Surv(time, event) ~ 1,
                       data = data_sim,
                       distr = "gompertz",
                       method = "hmc",
                       priors = list(gom = list(a_alpha = 0.1,
                                                b_alpha = 0.1)))
```

Lastly, we can run the blending step.

```{r}
blend_interv <- list(min = 48, max = 150)
beta_params <- list(alpha = 3, beta = 3)

ble_Surv <- blendsurv(obs_Surv, ext_Surv, blend_interv, beta_params)
```

We can visualise all of the curves.

```{r}
plot(ble_Surv)
```

## Two HMC survHE fits

In the same way as above we fit two survival curves and then blend them together.
In this case we use two exponential curves using the survHE package to fit them.

```{r}
obs_Surv2 <- fit.models(formula = Surv(death_t, death) ~ 1,
                        data = dat_FCR,
                        distr = "exponential",
                        method = "hmc")

ext_Surv2 <- fit.models(formula = Surv(time, event) ~ 1,
                       data = data_sim,
                       distr = "exponential",
                       method = "hmc")
```

```{r}
ble_Surv2 <- blendsurv(obs_Surv2, ext_Surv2, blend_interv, beta_params)

# kaplan-meier
km <- survfit(Surv(death_t, death) ~ 1, data = dat_FCR)

plot(ble_Surv2) +
  geom_line(aes(km$time, km$surv, colour = "Kaplan-Meier"),
            size = 1.25, linetype = "dashed")
```

## flexsurv frequentist background fit

The last example is for an HMC and frequentist survival model using the flexsurv package directly.

```{r}
obs_Surv3 <- fit.models(formula = Surv(death_t, death) ~ 1,
                        data = dat_FCR,
                        distr = "exponential",
                        method = "hmc")

ext_Surv3 <- flexsurv::flexsurvreg(formula = Surv(time, event) ~ 1,
                       data = data_sim,
                       dist = "gompertz")

ble_Surv3 <- blendsurv(obs_Surv3, ext_Surv3, blend_interv, beta_params)

plot(ble_Surv3)
```


## Piecewise uniform CDF

In the example above we sample from separate uniform distributions in between the times specified by the user.
These data are then used to fit a parametric survival curve which is use in the blending step.

As an alternative - rather than fitting to sampled data - we could simply use the underlying survival curve directly i.e. the 'curve' consisting of uniform segments.

We provide the function `pw_unif_surv()` to create a single piecewise uniform survival curve.

```{r}
p <- c(0.05, 0)
times <- c(144, 180)
surv <- pw_unif_surv(p, times, epsilon = 1)

plot(surv, type = "l")
```

We can now simply blend the two curves together in the usual way.
Because this is a single curve then there is no uncertainty associated with it.

```{r}
blend_interv <- list(min = 48, max = 150)
beta_params <- list(alpha = 3, beta = 3)

ble_Surv <- blendsurv(obs_Surv, surv$S, blend_interv, beta_params)

plot(ble_Surv)
```

Further, one way in which we can include uncertainty on the external curve is by sampling from a Dirichlet distribution on the user-provided survival probabilities.
If we use the probabilities as the parameter values of the Dirichlet distribution then the degree of certainty can be thought of as a scaling factor on these.
That is for a larger scaling, i.e. taking a product with a large value, is equivalent to more certainty and vice-versa using a smaller scaling is equivalent to more uncertainty.
Intuitively in a Bayesian context, we can view a hyperprior vector $p$ as pseudocounts, i.e. as representing the number of observations in each category that we have already seen.

First let us see for a moderate degree of uncertainty.

```{r}
blend_interv <- list(min = 30, max = 50)
beta_params <- list(alpha = 3, beta = 3)

surv <- sample_pw_unif_surv(p = c(0.8, 0.3, 0.2, 0),
                            times = c(10, 20, 30, 48),
                            n = 100, epsilon = 0.5)

matplot(surv, type = "l")

ble_Surv <- blendsurv(obs_Surv, surv, blend_interv, beta_params)

plot(ble_Surv)
```

For a small amount of uncertainty.

```{r}
surv <- sample_pw_unif_surv(p = c(0.8, 0.3, 0.2, 0),
                            times = c(10, 20, 30, 48),
                            n = 100, epsilon = 0.5, sn = 100)

matplot(surv, type = "l")

ble_Surv <- blendsurv(obs_Surv, surv, blend_interv, beta_params)

plot(ble_Surv)
```

For a large amount of uncertainty.

```{r}
surv <- sample_pw_unif_surv(p = c(0.8, 0.3, 0.2, 0),
                            times = c(10, 20, 30, 48),
                            n = 100, epsilon = 0.5, sn = 3)

matplot(surv, type = "l")

ble_Surv <- blendsurv(obs_Surv, surv, blend_interv, beta_params)

plot(ble_Surv)
```

